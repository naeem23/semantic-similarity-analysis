# ML 24/25-09 Semantic Similarity Analysis of Textual Data

This project is designed to analyze the semantic similarity between textual data (words/phrases or documents) using OpenAI's embedding models. It calculates cosine similarity scores for pairs of embeddings generated by three different OpenAI models: 'text-embedding-ada-002', 'text-embedding-3-small', and 'text-embedding-3-large'. The results are saved in a CSV file, and a Python script is provided to visualize the similarity scores using a scatter plot.

# Features
   * Textual Data Comparison: Compare semantic similarity at the Word or Phrase Level or Document Level.

   * Multiple Input Options: Accepts multiple source and reference inputs for comparison.

   * Folder Input Support: Allows users to input directories for document-level comparison.
   
   * Similarity Score: Calculates similarity scores using cosine similarity for three embedding models.

   * CSV Output: Saves similarity scores in a CSV file (`similarity_results.csv`) in the `Output` folder and in `bin\Debug\net9.0` folder with columns: Source, Reference, Score_Ada, Score_Small, Score_Large. Also creates two other CSV files name `source_scalar_values.csv` and `reference_scalar_values.csv` in the same directory to store scalar value of sources and references.

   * Visualization: Provides a Python script (`similarity-score-visualization.py`) to generate a bar chart to visualize the similarity scores of a reference (word/phrase or document) with other sources (word/phrases or documents) in terms of 3 different models. Another python script (`scalar-value-visualization.py`) to visualize scalar values of source vs reference. 


# Prerequisites

Before running the project, ensure you have the following installed:

   * .NET Framework: The project is built using the .NET Framework version 9.0.

   * OpenAI API Key: You need an OpenAI API key to generate embeddings. Add your API key to the configuration file (.env).

   * Python: Required for running the visualization script. This project uses Python 3.10.6.
   
   * NuGet Packages: The project uses the following packages. Install them via ```dotnet add package``` or restore them automatically with ```dotnet restore```:  

     * Core Packages:  
        CsvHelper by Josh Close (for reading writing CSV files)  
        DotNetEnv by DotNetEnv (to load environment variable from .env files)  
        itext7 by Apryse Software (for PDF parsing)  
        Newtonsoft.Json by James Newton-King (for JSON serialization)  
        OpenAI by OpenAI (for OpenAI service API)  
        RestSharp by .NET Foundation and Contributors (Simple REST and HTTP API Client)  
        Xceed.Words.NET by Xceed (for Microsoft Word documents parsing)  

      * Testing Packages (for test projects):  
        Microsoft.NET.Test.Sdk (by Microsoft)  
        MSTest.TestAdapter (by Microsoft)  
        MSTest.TestFramework (by Microsoft)  
        xunit (by jnewkirk,bradwilson)  
        xunit.runner.visualstudio (by jnewkirk,bradwilson)  
        Moq (by Daniel Cazzulino, kzu)  
        coverlet.collector (by tonerdo)  
    
   * Python Dependencies: Install the required Python packages using the python-requirements.txt file.


# Installation

1. Clone the repository:
   ```
   git clone https://github.com/naeem23/semantic-similarity-analysis.git
   cd semantic-similarity-analysis
   ```

2. Set Up Environment Variables:
Copy the .env.example file to .env in both the main project and test project directories.
Replace YOUR_API_KEY_HERE with your actual OpenAI API key in both .env files.
   ```
   OPENAI_API_KEY=your_api_key_here
   ```

4. Install the required .NET packages:
Restore the packages using:
    ```
    dotnet restore
    ```
Alternatively, install packages manually (if needed):  
Core packages:  
```
dotnet add package CsvHelper
dotnet add package DotNetEnv
dotnet add package itext7
dotnet add package Newtonsoft.Json
dotnet add package OpenAI
dotnet add package RestSharp
dotnet add package Xceed.Words.NET
```  

Testing packages:  
```
dotnet add package Microsoft.NET.Test.Sdk
dotnet add package MSTest.TestAdapter
dotnet add package MSTest.TestFramework
dotnet add package xunit
dotnet add package xunit.runner.visualstudio
dotnet add package Moq
dotnet add package coverlet.collector
```

6. Install Python dependencies:
   ```
   pip install -r python-requirements.txt
   ```


# Usage

   1. Run the .NET Project:
      *  Navigate to the project directory and run the project:
      ```
      dotnet run
      ```
      
      * The program will prompt you to choose between word/phrase level or document level comparison.

      * For word/phrase level, enter multiple source and reference inputs (one per line).

      * For document level, provide the paths to the source and reference folders.

      * The similarity scores will be calculated and saved in the `similarity_results.csv` file.

      * Scalar values of each source and reference embedding will be saved in `source_scalar_values.csv` and `reference_scalar_values.csv` repectively.

   2. Visualize the Results:
      * Navigate to the Python folder:
        ```
        cd Python
        ```
         
      * Run following python command to visualize similarity score of a reference word/phrase or documents against some source word/phrase or documents:
        ```
        python similarity-score-visualization.py
        ```

      * Run python command to visualize scalar values of source and reference word/phrase or documents:
        ```
        python scalar-value-visualization.py
        ```

      * Each time you run the python script a web server will open in your browser, displaying bar chart for similarity scores and  scatter plot for scalar values.


# Project Folder Structure
  ![image](https://github.com/user-attachments/assets/d6177a2d-5e3b-4945-8ec2-0cc494ceb4eb)


# Configuration

   * Add your OpenAI API key to the appropriate configuration file (e.g., .env).

   * Modify the Output folder path in the code if needed.


# Example
Input:
* Word/Phrase Level:  
Source: "machine learning", "artificial intelligence"  
Reference: "deep learning", "neural networks"  
![image](https://github.com/user-attachments/assets/37bc3e02-4136-4552-8334-a61a609cb88f)  

* Document Level:  
Source Folder: `G:\FUAS\SE\semantic-similarity-analysis\SemanticSimilarity\Input\Sources`  
Reference Folder: `G:\FUAS\SE\semantic-similarity-analysis\SemanticSimilarity\Input\References`  
![image](https://github.com/user-attachments/assets/4b73bd81-0781-454e-9719-568f94ed9511)  


# Output:
The similarity_results.csv file will contain:  
| Source  | Reference | Score_Ada | Score_Small | Score_Large |
| ------------- | ------------- | ------------- | ------------- | ------------- |
| machine learning  | deep learning  | 0.89 | 0.65 | 0.71 |
| machine learning | neural networks | 0.83 | 0.51 | 0.51 | 
| artificial intelligence | deep learning | 0.85 | 0.47 | 0.49 |
| artificial intelligence  | neural networks | 0.82 | 0.41 | 0.43 |


# Visualization:
The scatter plot will display the similarity scores for each model.  
![image](https://github.com/user-attachments/assets/4391d5cd-08cb-446c-a8f7-65b96e85b483)


# Test Project
The test project is included to ensure the correctness of the main project. It uses the same .env file for the OpenAI API key.

   1. Run Tests:
        Navigate to the tests directory and run the tests:
        ```
        dotnet test
        ```


# Contributing
   Contributions are welcome! Please open an issue or submit a pull request for any improvements or bug fixes.


# License
   This project is licensed under the MIT License. See the LICENSE file for details.
